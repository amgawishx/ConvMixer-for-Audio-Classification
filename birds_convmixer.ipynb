{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"markdown","source":"# Auditory Classification of Birds using ConvMixer Architecture.\n##### author: amgawishx@github.com","metadata":{}},{"cell_type":"markdown","source":"### Section 1\nThis section contains library imports and hyperparameters of the entire notebook,\nyou can customize any aspect of the code from here.","metadata":{}},{"cell_type":"code","source":"### Section 1\nimport os # for reading files & pathes\nimport tqdm # for pretty progress bar\n\n# the main components\nimport torch\nimport torchaudio\nimport torchvision\nimport torch.nn as nn\n\nimport polars as ps # for reading & manipulating csv files\nimport matplotlib.pyplot as plt # visualization\n\n# helpers to manage preprocessing & memory\nfrom torch.cuda import OutOfMemoryError\nfrom torch.utils.data import Dataset\nfrom torch.utils.data import DataLoader\nfrom torch.nn.parallel import DataParallel\nfrom torch.nn.utils.rnn import pad_sequence\n\nfrom functools import reduce\n\ndata_path = \"/kaggle/input/birdclef-2023/train_audio/\"\nmeta_path = \"/kaggle/input/birdclef-2023/train_metadata.csv\"\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n\nsr = 32000 # audio sample rate\nnfft = 1024 # number of FFT points, 2^N, N = 10\nno_classes = 264 # number of damn birds\nlr = 1e-3 # learning\ndims = 32 # latent output dimension of the network\ndepth = 10 # depth of the the network\nbsize = 32 # batch size\nepochs = 1000 # number of epochs (theoretically)\nn_fc = 1024 # the width of the fully connected layers","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","execution":{"iopub.status.busy":"2023-04-04T18:19:27.252293Z","iopub.execute_input":"2023-04-04T18:19:27.252595Z","iopub.status.idle":"2023-04-04T18:19:30.629686Z","shell.execute_reply.started":"2023-04-04T18:19:27.252567Z","shell.execute_reply":"2023-04-04T18:19:30.628431Z"},"trusted":true},"execution_count":1,"outputs":[]},{"cell_type":"markdown","source":"### Section 2\nThis section defines the class used to load the training data and the\ncollate function used to preprocess it; Since every audio file is of variable length,\nwe need a way to standarize the waveform length, and we do this in the\n`len_collate` function via the `rnn.pad_sequence` operation.","metadata":{}},{"cell_type":"code","source":"### Section 2\n\nclass BirdsDataset(Dataset):\n    \"\"\"\n    A class to preprocess and load training audio data\n    ---\n    meta_path: str -> path to the train metadata file.\n    train_dir: str -> path to the training data files.\n    transform: nn.Module -> a transform to perform on the data while loading.\n    \"\"\"\n    def __init__(self, meta_path: str,\n                 train_dir: str,\n                 sample_rate: int = 32000,\n                 transform: nn.Module = None,\n                 device: str = \"cpu\") -> None:\n        named_labels = list(ps.read_csv(meta_path)[:,0].unique()) # get class names\n        self.data = []\n        # populate self.data with pairs of an audio file and its label\n        for label in named_labels:\n            files_path = os.path.join(train_dir,label)\n            files = os.listdir(files_path)\n            for file in files:\n                self.data.append((os.path.join(files_path, file),\n                                  named_labels.index(label)))\n        self.dir = train_dir\n        self.transform = transform\n        self.sample_rate = sample_rate\n        self.device = device\n        \n    def __len__(self) -> int:\n        return len(self.data)\n    \n    def __getitem__(self, idx) -> tuple:\n        audio = self._audio_loader(self.data[idx][0])\n        label = self.data[idx][1]\n        if self.transform:\n            audio = self.transform(audio)\n        return audio, label\n        \n    def _audio_loader(self, audio_path):\n        waveform, sr = torchaudio.load(audio_path, normalize = True)\n        resampler = torchaudio.transforms.Resample(sr, self.sample_rate)\n        return resampler(waveform[0,:].to(self.device))\n\ndef len_collate(batch, device):\n    \"\"\"\n    A function to pass it to dataloader to pad each batch of\n    time series to the same length, loaded onto `device`.\n    \"\"\"\n    waveforms, labels = [], []\n    for waveform, label in batch:\n        try:\n            waveform = torch.transpose(waveform,0,1) # swap frequency & time dimensions\n        except IndexError:\n            pass\n        waveforms.append(waveform)\n        labels.append(torch.tensor(label).to(device))\n    padded_waveforms = torch.stack([waveform.reshape((1, *waveform.shape))\n                          for waveform in pad_sequence(waveforms, batch_first = True)], 0)\n    del waveforms, batch\n    return padded_waveforms, torch.stack(labels, dim = 0)","metadata":{"execution":{"iopub.status.busy":"2023-04-04T18:19:30.632041Z","iopub.execute_input":"2023-04-04T18:19:30.632735Z","iopub.status.idle":"2023-04-04T18:19:30.647073Z","shell.execute_reply.started":"2023-04-04T18:19:30.632698Z","shell.execute_reply":"2023-04-04T18:19:30.645978Z"},"trusted":true},"execution_count":2,"outputs":[]},{"cell_type":"markdown","source":"### Section 3\nHere we define the class model as defined per the paper titled: [Patches is All You Need?](https://arxiv.org/pdf/2201.09792.pdf), the paper defines a convolutional neural network that uses convolutional layers to mix between spatial & channel-wise information to better understand the underlying representation of the data, akin to puzzle solving.\n\n![The ConvMixer model](https://raw.githubusercontent.com/IbrahimSobh/Transformers/main/images/conmixer01.png)\n\nWe also define the MEL Spectrogram transform that translates an audio problem to a vision one by producing an image like the following:\n\n![MEL Spectrogram](https://librosa.org/doc/main/_images/librosa-feature-melspectrogram-1.png)","metadata":{}},{"cell_type":"code","source":"### Section 3\n\nclass Residual(nn.Module):\n    \"\"\"\n    A skip-connection block used in the ConvMixer model.\n    \"\"\"\n    def __init__(self, fn):\n        super().__init__()\n        self.fn = fn\n\n    def forward(self, x):\n        return self.fn(x) + x\n\ndef SpectrumTransform(nfft):\n    \"\"\"\n    Performs a MEL Spectrogram transform on input waveform.\n    Used to extract auditory features of a sound file.\n    \"\"\"\n    return nn.Sequential(torchaudio.transforms.Spectrogram(n_fft=nfft, power=2),\n                    torchaudio.transforms.MelScale(n_stft = nfft // 2 + 1,\n                                                  sample_rate = sr),\n                   torchaudio.transforms.AmplitudeToDB()).to(device)\n\ndef ConvMixer(dim, depth, no_classes,\n              resize_shape = (100, 200),\n              kernel_size = 9, patch_size = 5):\n    \"\"\"\n    The main classifier of the model, takes an input\n    a MEL Spectrogram in dB and performs convolutional\n    spatial and channel mixing between the inputs then pass\n    it to a classification head.\n    \"\"\"\n    return nn.Sequential(\n        # preprocessing tail\n        torchvision.transforms.Resize(resize_shape),\n        nn.Conv2d(1, dim, kernel_size=patch_size, stride=patch_size),\n        nn.GELU(),\n        nn.BatchNorm2d(dim),\n        \n        # mixer layers\n        *[nn.Sequential(\n            Residual(nn.Sequential(\n                nn.Conv2d(dim, dim, kernel_size,\n                          groups=dim, padding=\"same\"),\n                nn.GELU(),\n                nn.BatchNorm2d(dim)\n            )),\n            nn.Conv2d(dim, dim, kernel_size=1),\n            nn.GELU(),\n            nn.BatchNorm2d(dim)\n        ) for _ in range(depth)],\n        \n        # classification head\n        nn.AdaptiveAvgPool2d((1, 1)),\n        nn.Flatten(),\n        nn.Linear(dim, n_fc),\n        nn.ReLU(),\n        nn.Linear(n_fc, no_classes),\n        nn.Softmax(),\n    )","metadata":{"execution":{"iopub.status.busy":"2023-04-04T18:19:30.649092Z","iopub.execute_input":"2023-04-04T18:19:30.649931Z","iopub.status.idle":"2023-04-04T18:19:30.663056Z","shell.execute_reply.started":"2023-04-04T18:19:30.649892Z","shell.execute_reply":"2023-04-04T18:19:30.662142Z"},"trusted":true},"execution_count":3,"outputs":[]},{"cell_type":"markdown","source":"### Section 4\nIn this section we define helper functions to evaluate metrics of performance of our network and train it.","metadata":{}},{"cell_type":"code","source":"### Section 4\n\ndef accuracy(output, target):\n    \"\"\"\n    A helper function to evalute the accuracy of\n    model's prediction over a batch of inputs.\n    \"\"\"\n    with torch.no_grad():\n        pred = output.argmax(dim=1)\n        correct = (pred == target.argmax(dim=1)).sum().item()\n        total = target.size(0)\n    return correct / total * 100\n\ndef train(dataloader, model, loss_fn, optimizer):\n    \"\"\"\n    The function used to train a model on data from dataloder\n    given a loss function and an optimizer.\n    \"\"\"\n    data = tqdm.tqdm(enumerate(dataloader)) # get a progress bar\n    size = len(dataloader.dataset)\n    accuracies = [] # store accuracy history\n    losses = [] # store losses history\n    model.train() # enable training mode for the network\n    for batch, (X, y) in data:\n        y = torch.nn.functional.one_hot(y, num_classes=no_classes).float() # one-hot encode the classes\n        pred = model(X)\n        loss = loss_fn(pred, y)\n        \n        # backpropagate\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n        \n        # get metrics\n        loss, current = loss.item(), batch * len(X)\n        with torch.no_grad():\n            losses.append(loss)\n            accuracies.append(accuracy(pred, y))\n            mean_accuracy = sum(accuracies)/len(accuracies)\n            mean_loss = sum(losses)/len(losses)\n        data.set_description(f\"loss: {mean_loss:<7f} [{current:>5d}/{size:>5d}],\"+\\\n              f\" accuracy: {mean_accuracy:<3f}%\")\n        # clear memory\n        del X, y\n        torch.cuda.empty_cache()\n    return losses, accuracies # return losses & accuracies for performance graph","metadata":{"execution":{"iopub.status.busy":"2023-04-04T18:19:30.665821Z","iopub.execute_input":"2023-04-04T18:19:30.666695Z","iopub.status.idle":"2023-04-04T18:19:30.679141Z","shell.execute_reply.started":"2023-04-04T18:19:30.666651Z","shell.execute_reply":"2023-04-04T18:19:30.678350Z"},"trusted":true},"execution_count":4,"outputs":[]},{"cell_type":"markdown","source":"### Section 5\nThe final section where we load our data, instantiate our model and start the training loop; Training originally has been performed on 2 GPU T4 with 16GB each and system RAM of 13GB. If you are using a single GPU, then just remove the `DataParallel` line.","metadata":{}},{"cell_type":"code","source":"### Section 5\n\ndata = BirdsDataset(meta_path, data_path,\n                    sample_rate = sr, device = device) # load training dataset\n\ndataloader = DataLoader(data, batch_size = bsize,\n                        collate_fn = lambda x : len_collate(x, device), \n                        shuffle = True) # preprocess and prepare it for loading\n\nmodel = nn.Sequential(\n    SpectrumTransform(nfft),\n    ConvMixer(dims, depth, no_classes)\n).to(device) # instantiate the model\n\nmodel = DataParallel(model, device_ids=[0, 1]) # parallel train the model on cuda:0 & cuda:1\n\n# run the training loop with cross-entropy loss and RMS Propagation optimizer\nloss_fn = nn.CrossEntropyLoss()\noptimizer = torch.optim.RMSprop(model.parameters(), lr = lr)\ntotal_losses = []\ntotal_accuracies = []\nfor _ in range(epochs):\n    if _%10 == 0: print(f\"Epoch {_+1}:\")\n    try:\n        current_loss, current_accuracy = train(dataloader, model, loss_fn, optimizer)\n        total_losses += current_loss\n        total_accuracies += current_accuracy\n    except OutOfMemoryError: # sometimes the gpu's memory get congested so we need to clear it\n           torch.cuda.empty_cache()","metadata":{"execution":{"iopub.status.busy":"2023-04-04T18:42:35.688963Z","iopub.execute_input":"2023-04-04T18:42:35.689828Z","iopub.status.idle":"2023-04-04T18:42:35.937459Z","shell.execute_reply.started":"2023-04-04T18:42:35.689764Z","shell.execute_reply":"2023-04-04T18:42:35.936227Z"},"trusted":true},"execution_count":8,"outputs":[]}]}